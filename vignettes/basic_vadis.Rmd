---
title: "Introduction to the VADIS method"
author: "Jason Grafmiller"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    df_print: kable
    fig_caption: yes
    css: pandoc.css
vignette: >
  %\VignetteIndexEntry{Basic application of VADIS method}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: vadis_refs.bib
---

```{r setup, include = FALSE}
library(knitr)
knitr::opts_chunk$set(
  collapse = TRUE,
  error = FALSE,
  message = FALSE,
  comment = "#>"
)
knitr::opts_knit$set(
  root.dir = normalizePath('../'), # set working directory as root
  cache.path = "vignettes/basic_vadis_cache/") 
```

This vignette introduces the basics of the Variation-Based Distance & Similarity Modeling (**VADIS**) method and demonstrates how to use the core functions in the `VADIS` package. The VADIS method builds upon techniques in comparative sociolinguistics and quantitative dialectometry for quantifying the similarity between varieties and dialects as captured by correspondences among the ways in which language users choose between different ways of saying the same thing. For details of the method and theoretical motivation see @SzmrecsanyiInprep and @GrafmillerToappear. 

The basic libraries you'll need for this vignette.

```{r libs, comment=F, message=FALSE, error=FALSE}
library(tidyverse) # for data wrangling
library(lme4) # for regression models
library(party) # for random forests
library(phangorn) # for neighborNets
```

Install and load the `VADIS` package.

```{r eval = F}
devtools::install_github("jasongraf1/VADIS")
library(VADIS)
```

```{r echo = F}
library(VADIS)
```

# Setup

For this tutorial, we will examine how the constraints on the placement of postverbal particles, e.g. *off* in examples (a) and (b), vary across different varities of English. 

(a) *She uses a pair of long metal scissors to **cut off** a length of it.* [continuous order]
(b) *She uses a pair of long metal scissors to **cut** a length of it **off**.* [split order] 

We'll use a version of the dataset of particle verb constructions used by @GrafmillerToappear. Full details of the dataset can be found here: [https://osf.io/x8vyw/](https://osf.io/x8vyw/). 

```{r}
# call the dataset 
pv <- particle_verbs_short
names(pv)
```

This dataset contains approximately 11000 tokens from nine different varities of English from around the world. 

```{r fig.height = 4, fig.width=6}
ggplot(pv, aes(Variety, fill = Response)) +
  geom_bar(position = "dodge")
```



# VADIS steps

The VADIS method is designed to measure the degree of (dis)similarity among "variable grammars" of different dialects or varieties, where a variable grammar is understood as the set of constraints (a.k.a. predictors or "conditioning factors") governing the choice between two or more linguistic variants. Variants can be individual lexical items (*sneakers* vs. *trainers* vs. *tennis shoes*), grammatical constructions (*give me the book* vs. *give the book to me*), or phonetic realizations of a particular phoneme (e.g. [ʊ] vs. [ʌ] pronunciations of the **<span style="font-family:Times New Roman, Times, serif">STRUT</span>** vowel).  

The method takes inspiration from comparative sociolinguistics [see e.g. @Tagliamonte2013], which evaluates the relatedness between varieties and dialects based on how similar the conditioning of variation is in these varieties. Comparative sociolinguists rely on three lines of evidence to determine relatedness:  

1. Are the same constraints significant across varieties? 
2. Do the constraints have the same strength across varieties? 
3. Is the constraint hierarchy similar?

Below we'll go through the steps for calculating and assessing each of these lines of evidence. 

#### Splitting the dataset

The first thing we need to do is to get the data into the necessary format. With VADIS we analyze variables across different datasest separately, and so the basic data object that the `VADIS` functions work with is a **list of dataframes**.

For this tutorial, we're modeling particle placement across nine varieties separately, so we first split the data up into nine individual datasets stored together as a list.

```{r}
# create list of dataframes
data_list <- split(pv, pv$Variety, drop = TRUE) # drop unused levels
names(data_list)
```

Now we're ready to begin.

## Step 1

In the first step we identify the most important constraints on our variable. We know from prior research that many different factors influence the choice of particle placement, including:

- Length, definiteness, concreteness, and discourse accessibility of the direct object (Note: personal pronoun direct objects are excluded from this dataset)
- The semantic idiomaticity of the verb-particle
- The presence of a directional PP following the verb phrase 
- predictability of the particle given the verb, and the predictability of the verb given the particle
- Prior use of V-NP-P or V-P-NP orders
- The register, genre and/or mode (e.g. informal speech has much more V-NP-P tokens than formal writing) 

These are the predictors we'll include in our variable grammar models. We'll define our model formula using this set.

```{r}
f1 <- Response ~ DirObjWordLength + DirObjDefiniteness + DirObjGivenness + 
  DirObjConcreteness + DirObjThematicity + DirectionalPP + PrimeType + 
  Semantics + Surprisal.P + Surprisal.V + Register
```

## Step 2

We now fit logistic regression models to the data (sub)sets. We'll use fixed-effects only models for this vignette since they are faster to compute, but I give an example of the code for a mixed-effects model below.

Next we simply loop over the datasets in `data_list` and fit a model to each one. Before fitting we'll center and standardize our model inputs [see @Gelman2008]. 

**IMPORTANT:** Standardizing inputs is essential for reliable calculation of the similarity coefficients in line 2 (Step 4). See `?stand` for details. However, **only fixed effects predictors should be standardized**. There are a number of ways to do this but we've included a function `stand()` that will take care of this easily (see `help(stand)` for details)

```{r}
glm_list <- vector("list") # empty list to store our models 
for (i in seq_along(data_list)){
  d <- data_list[[i]]
  # now standardize the model fixed effects inputs before fitting.
  d_std <- stand(d, cols = f1) # use the fitting function for convenience
  # fit the model
  glm_list[[i]] <- glm(f1, data = d_std, family = binomial, x = TRUE) # note the x = TRUE
}
names(glm_list) <- names(data_list) # add names to the list of models
```

Notice the inclusion of the `x = TRUE` argument in the `glm()` function. This tells it to store the data matrix in the model object, and this is necessary for the VADIS functions to work with `glm` models. This is not necessary for models fit with `glmer`.

Mixed-effects models are run in much the same fashion. We just have to define a new formula and different modeling function. Again, we won't use these models in this vignette, since mixed-models can take much longer to compute.

```{r}
# update formula with by-verb and by-particle random intercepts 
f2 <- update(f1, .~ (1|Verb) + (1|Particle) + .)
f2
```

```{r eval = F}
glmer_list <- vector("list")
for (i in seq_along(data_list)){
  d <- data_list[[i]]
  # standardize the model inputs, excluding the response and random effects
  d_std <- stand(d, cols = f2) # use the fitting function for convenience
  # fit the model
  glmer_list[[i]] <- glmer(f2, data = d_std, family = binomial, # set optimizer controls to help convergence 
    control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e7)))
  rm(d, d_std) # remove datasets
}
names(glmer_list) <- names(data_list)
```

### Checking fits

Before moving on, we should check that the models fit the data well, and that we don't have any issues with collinearity or other signs of trouble. The `summary_stats()` function gives a number of statistics to consider.

```{r}
summary_stats(glm_list) %>% 
  round(3)
```

These are:

- `N`: the total number of observations
- `baseline`: the baseline accuracy, i.e. the proportion of the most frequent response
- `predicted.corr`: the proportion of observations predicted correctly
- `C`: the concordance index *C*, a.k.a. as the area under the receiver operating characteristic (ROC) curve. Values below .8 suggest poor model discrimination
- `AIC`: the model's Akaike Information Criterion
- `Max.VIF`: the maximum variance inflation factor for the model's parameter estimates. Values above 10 suggest potentially troublesome collinearity among predictors
- `kappa`: the condition number indicating overall data multicollinearity. Values < 6 indicate no collinearity; ~15 indicate medium collinearity; > 30 indicate a worrysome degree of collinearity [@Baayen2008, 198-200].
- `HosLem.p`: the p-value of the Hosmer-Lemeshow test for goodness of fit [@Hosmer1980]. This is a popular though controversial goodness of fit test for logistic regression [see @Harrell2015b, 236-237]. Low values (*p* < .05) indicate potential problems.

All the models appear to discriminate between the two responses fairly well, though some of our models may have some issues with model fit. We'll set this aside for the time being.


## Step 3 

In step 3 we look at the first line of evidence: "Are the same constraints significant across varieties?"

We do this with the `vadis_line1()` function. This function returns a list with three elements.

- `signif.table`: A table of predictor significance values, including the intercept. 1 = significant, 0 = non-significant
- `distance.matrix`: A distance matrix derived from `signif.table`. For line 1 distances are [Hamming distances](https://en.wikipedia.org/wiki/Hamming_distance), which in the case of binary data is equivalent to the Manhattan or squared Euclidean distance (the intercept is *excluded* when calculating the distances). In such cases these distances are equivalent to the number of constraints onr which the two varieties disagree about their statistical significance. These distances are then normalized by dividing by the total number of constraints, i.e. the maximum number of constraints on which two varieties could disagree.
- `similarity.scores`: A table of scores measuring the mean similarity for each variety to all the others. See @SzmrecsanyiInprep for description of how these are calculated.

The `path` argument tells the function where to save the output. The default setting saves a file `vadis_line1_output_2018-Oct-30_15.04.rds` to the current working directory. Note that the output of the function is a list object, so it saves it as an R data object file (`.rds`). Setting `path = FALSE` tells it not to save anything. 

```{r}
signif_line <- vadis_line1(glm_list, path = FALSE)
```

Examine each of the elements.

```{r}
signif_line$signif.table
```

```{r}
signif_line$distance.matrix %>% 
  round(3)
```

```{r}
signif_line$similarity.scores %>% 
  arrange(desc(Similarity)) # sort by similarity
```

Individual components can be saved as .csv (or.txt) files like so.

```{r eval = F}
write.csv(signif_line$signif.table, 
          file = "line1_significance_table.csv")

write.csv(as.matrix(signif_line$distance.matrix), 
          file = "line1_distance_matrix.csv")

write.csv(signif_line$similarity.scores, 
          file = "line1_similarity_scores.csv")
```


## Step 4

In the next step we look at the second line of evidence. We do this with the `vadis_line2()` function. This function also returns a list with three elements.

- `coef.table`: A table of predictor coefficients from the regression models, including the intercept.
- `distance.matrix`: A distance matrix based on `coef.table`. For this function distances are [Euclidean distances](https://en.wikipedia.org/wiki/Euclidean_distance), normalized to a maximum reasonable distance (see below). The intercept is *excluded* when calculating the distances.
- `similarity.scores`: A table of coefficients measured as the mean similarity for each variety to all the others. See @SzmrecsanyiInprep for description of how these are calculated.

```{r}
coef_line <- vadis_line2(glm_list, path = FALSE)
```

```{r}
coef_line$coef.table %>% 
  round(3)
```

```{r}
coef_line$distance.matrix %>% 
  round(3)
```

```{r}
coef_line$similarity.scores %>% 
  arrange(desc(Similarity))
```


Note that both `vadis_line1()` and `vadis_line2()` work the same with `glm` and `glmer` models.

## Step 5

In this step we calculate the conditional random forest models of each dataset. From these we'll then calculate variable importance rankings which comprise the third line of evidence. 

Now we loop over the datasets in `data_list` and apply the modeling function just like we did with the regression models. In this tutorial we'll use the `cforest()` function in the `party` package, but the `VADIS` package also supports random forests generated with `ranger`. **NOTE**: The `controls` options below are just the defaults. You will need to adapt these to suit your needs for a given dataset 

```{r crf_list}
crf_list <- vector("list") # empty list to store our models
for (i in seq_along(data_list)){
  d <- data_list[[i]]
  # fit the random forest and add it to the list
  crf_list[[i]] <- cforest(f1, d, controls = cforest_unbiased(ntree = 500, mtry = 3))
}
```

This gives us a list of random forests models similar to our list of regression models above.

If for-loops aren't your style, you can use `lapply()` instead.  

```{r crf_list2, cache = F, eval = F}
crf_list <- lapply(data_list, 
  FUN = function(d) cforest(f1, d, controls = cforest_unbiased(ntree = 500, mtry = 3)))
```

**WARNING**: Random forest objects can take a long time to run, and the resulting objects can be quite large, especially those generated with `party` (e.g. the `crf_list` above is 559 Mb in size. With 1000 trees it doubles to 1.1 Gb). Processing many forests with large numbers of trees can tax your CPU and RAM and slow down R's functioning. It's recommended that you save these objects to a file to be called as needed.

### Checking fits

Before moving on, we should again check that the models fit the data well, and that we don't have any issues with collinearity or other signs of trouble. The `summary_stats()` function gives a subset of the statistics we saw before, mainly because random forest models don't provide us with these statistics. Additional arguments are also needed since the model objects don't contain the necessary information.

```{r}
summary_stats(crf_list, data_list, response = "Response") %>% 
  round(3)
```


## Step 6

In this next step we look at the third line of evidence. We do this with the `vadis_line3()` function. This function also returns a list with, you guessed it, three elements.

- `rank.table`: A table of variable importance rankings.
- `distance.matrix`: A distance matrix based on `rank.table`. For this function distances are measured by calculating the pairwise Spearman rank correlation coefficients and subtracting from 1.
- `similarity.scores`: A table of mean similarity coeficients measured for each variety. These are equivalent to the Spearman rank correlations.

We then loop over the list of random forests and calculate the variable importance rankings. For `party` forests we use the `varimpAUC()` function, while `ranger` forests use the "permutation" method. With `party` forests we can also specify whether the rankings should be based on conditional importance scores, which are advised when there are many potentially correlated predictors [@Strobl2008]. Since conditional importance takes *much* longer to compute, we'll set it to `FALSE` (the default) here.

```{r line3, cache=F}
varimp_line <- vadis_line3(crf_list, path = FALSE, conditional = FALSE)
```

```{r}
varimp_line$distance.matrix %>% 
  round(3)
```

```{r}
varimp_line$similarity.scores %>% 
  arrange(desc(Similarity))
```


### Calculating maximum distance in line 2

In order to appropriately calculate line 2, we need to determine a maximum distance measure with which to normalize the pairwise distances. We define this distance based on what we know about the range of possible values the data is likely to have. We use the distance between two hypothetical varieties whose constraints have exactly the opposite effects. Such cases of constraint "flipping", i.e. a systematic reversal in the direction of constraint effects between two varieties, is very unlikely to happen in real world contexts, which is exactly the point. We additionally set the absolute size of all the constraints to a reasonable value to get two (hypothetical) varieties that are about as different from one another as we could expect any two varieties to be. We are really, really unlikely to come across a real world situation where two varieties’ grammars are more dissimilar than this.

For a comparison of grammars with *n* constraints, we define the maximum reasonable distance by taking the distance between two hypothetical varieties whose constraints all have an absolute effect size of ±1 but whose effect directions are the opposite of one another in the respective varities. For example, in a study comparing grammars with 10 constraints, we calculate the maximum reasonable distance by taking the euclidean distance between hypothetical varieties A and B that look like so.

```{r echo=F, eval = T}
d <- data.frame(
  a = rep(c(-1,1), each = 5),
  b = rep(c(1,-1), each = 5))
d
```

The maximum distance in this case is therefore d(A,B) = `r round(as.numeric(dist(t(d), "euclidean")), 2)`. To normalize our distance matrices we then divide the observed distances by this value to give distances within a range of 0 to 1. For the similarity scores we simply subtract the distances from 1 to give us a score where larger values represent greater average similarity.

We chose ±1 as the value for our hypothetical constraints since a change from -1 to 1 on the log odds scale represents an increase in probability of approximately 50 percentage points: *P* = logit^-1^(-1) = 0.27; *P* = logit^-1^(1) = 0.73. It seems very unlikely that we'd ever see such a big reversal in *every* constraint's effect between two related varieties. Larger values were also considered, e.g. ±2, but the larger the maximum distance, the less discrimination there is among the observed distances, and so we opted for slightly smaller values.

# Combining the 3 lines

We can merge the similarity scores across the three lines of evidence, and arrive at mean scores for each variety.

```{r}
mean_sims <- data.frame(
  line1 = signif_line$similarity.scores[,2], # get only the values in the 2nd column
  line2 = coef_line$similarity.scores[,2],
  line3 = varimp_line$similarity.scores[,2]
)
mean_sims$mean <- apply(mean_sims, 1, mean)
rownames(mean_sims) <- names(data_list)
round(mean_sims, 3)
```

From this we can calculate a mean of the mean similarity scores. This is our **Core Grammar Coefficient** for particle placement across these varieties [@SzmrecsanyiInprep]

```{r}
mean(mean_sims$mean)
```

We can also create a combined distance matrix from the three lines of evidence. We use the `fuse()` function in the `analogue` package.

```{r}
fused_dist <- analogue::fuse(signif_line$distance.matrix, 
                             coef_line$distance.matrix, 
                             varimp_line$distance.matrix)
round(fused_dist, 3)
```


# Visualization

Using the distance matrices from the different lines of evidence (or the combined distances), we can visualize the similarities among varieties in a number of ways.


## Clustering

One way we can visualize distances is through clustering methods. There are many different functions and packages for doing this, the simplest case probably being the `hclust()` function.

```{r hclustplot, fig.height=6, fig.width=7}
# Use the 2nd line of evidence
line2_clust <- hclust(coef_line$distance.matrix, method = "ward.D2")
plot(line2_clust, main = "Hierarchical clustering of line 2 distances")
```

We can look at the fused matrix combining the 3 lines of evidence, which gives us a slightly different picture.

```{r fig.height=6, fig.width=7}
hclust(fused_dist, method = "ward.D2") %>% 
  plot(main = "Hierarchical clustering of fused distances")
```


Other methods use divisive clustering (with `cluster::diana()`) or unrooted trees with neighbor joining methods (with `ape::nj()`).

```{r fig.height=6, fig.width=7}
cluster::diana(coef_line$distance.matrix) %>% 
  cluster::pltree(main = "Divisive clustering of line 2 distances")
```

```{r fig.height=6, fig.width=7}
ape::nj(coef_line$distance.matrix) %>% 
  plot(type = "u", main = "unrooted clustering of line 2 distances")
```

```{r fig.height=6, fig.width=7}
ape::nj(fused_dist) %>% 
  plot(type = "u", main = "unrooted clustering of fused distances")
```



## Multidimensional scaling

Distance matrices can also be fed into a multidimensional scaling analysis, which maps distances onto a 2 or 3 dimensional space. The closer the points are in this space, the more similar the varieties' grammars.

```{r}
line2_mds <- cmdscale(coef_line$distance.matrix, k = 3, eig = T) 
```

```{r mdsplot, fig.height=6, fig.width=7}
line2_mds[[1]] %>%
  as.data.frame() %>% 
  mutate(genres = rownames(.)) %>% 
  ggplot(aes(V1, V2, label = genres)) +
  geom_point() +
  geom_text(nudge_y = .01, size = 4)
```

One nice thing about MDS maps is that they can be represented in 2 or 3 dimensions. There are a number of methods for generating 3D plots. The simplest is probably `scatterplot3d`.

```{r fig.height=6, fig.width=7}
dd <- line2_mds[[1]] %>%
  as.data.frame() 
library(scatterplot3d)
with(dd, {
  scttr <- scatterplot3d(x = V1, y = V2, z = V3, type = "h", pch = 18)
  scttr_coords <- scttr$xyz.convert(V1, V2, V3)
  text(scttr_coords$x, scttr_coords$y, labels = rownames(dd), pos = 3)
  })
```

`plotly` is another powerful package for generating all kinds of interactive graphics. 

```{r fig.height=6, fig.width=7}
library(plotly)
dd %>% 
  mutate(Variety = rownames(.)) %>% 
  plot_ly() %>%
  add_trace(x = ~V1, y = ~V2, z = ~V3,
            type = "scatter3d", inherit = F,
            marker = list(size = 4),
            mode = "markers") %>%
  add_text(x = ~V1, y = ~V2, z = ~V3,
           text = ~ Variety,
           type = "scatter3d",
           mode = "markers",
           showlegend = FALSE)
```

We can also run and MDS based on the fused distance matrix.

```{r}
fused_mds <- cmdscale(fused_dist, k = 3, eig = T)
```

<!-- We can use a scree plot to see how many dimensions we might want to plot. -->

<!-- ```{r} -->
<!-- barplot(fused_mds$eig) -->
<!-- ``` -->

```{r fig.height=6, fig.width=7}
fused_mds[[1]] %>% # extract the coordinates
  as.data.frame() %>% 
  mutate(Variety = rownames(.)) %>% 
  plot_ly() %>%
  add_trace(x = ~V1, y = ~V2, z = ~V3,
            type = "scatter3d", inherit = F,
            marker = list(size = 4),
            mode = "markers") %>%
  add_text(x = ~V1, y = ~V2, z = ~V3,
           text = ~ Variety,
           type = "scatter3d",
           mode = "markers",
           showlegend = FALSE)
```



## NeighborNets

NeighborNet is yet another technique for constructing and visualizing phylogenetic relationships. The method is similar to other agglomerative clustering methods, e.g. neighbor-joining algorithms, but allows for "conflicting" signals. These conflicting signals are realized as web-like reticulations which indicate that the data support several possible tree structures [see @Dunn2008].  

```{r NNetplot, fig.height=6, fig.width=7}
line2_NNet <- phangorn::neighborNet(coef_line$distance.matrix)
plot(line2_NNet, "2D")
```

Look at NNet derived from the the fused distance matrix.

```{r NNetplot2, fig.height=6, fig.width=7}
phangorn::neighborNet(fused_dist) %>% 
  plot("2D")
```




## References


