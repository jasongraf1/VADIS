---
title: "Introduction to the VADIS method"
author: "Jason Grafmiller"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    df_print: kable
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Basic application of VADIS method}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: vadis_refs.bib
---

```{r setup, include = FALSE}
library(knitr)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_knit$set(root.dir = normalizePath('../')) # set working directory as root
```

This vignette introduces the basics of the Variation-Based Distance & Similarity Modeling (**VADIS**) method and demonstrates how to use the functions in the `PACKAGE` (Name suggestions?). The VADIS method builds upon techniques in comparative sociolinguistics and quantitative dialectometry for quantifying the similarity between varieties and dialects as captured by correspondences among the ways in which language users choose between different ways of saying the same thing. For details of the method and theoretical motivation see @SzmrecsanyiInprep and @GrafmillerToappear. 

The basic libraries you'll need for this vignette.

```{r libs, comment=F, message=FALSE, error=FALSE}
library(tidyverse) # for data wrangling
library(lme4) # for regression models
library(party) # for random forests
library(phangorn) # for neighborNets
```

Install and load the `PACKAGE`.

```{r eval = F}
devtools::install_github("jasongraf1/VADIS")
library(VADIS)
```

```{r echo = F}
library(VADIS)
```

# Setup

For this tutorial, we will examine how the constraints on the placement of postverbal particles, e.g. *off* in examples (a) and (b), vary across different varities of English. 

(a) *She uses a pair of long metal scissors to **cut off** a length of it.* [continuous order]
(b) *She uses a pair of long metal scissors to **cut** a length of it **off**.* [split order] 

We'll use a version of the dataset of particle verb constructions used by @GrafmillerToappear. Full details of the dataset can be found here: [https://osf.io/x8vyw/](https://osf.io/x8vyw/). 

```{r}
# call the dataset 
pv <- particle_verbs_short
names(pv)
```

This dataset contains approximately 1000 tokens from nine different varities of English from around the world. 

```{r fig.height = 4, fig.width=6}
ggplot(pv, aes(Variety, fill = Response)) +
  geom_bar(position = "dodge")
```



# VADIS steps

The VADIS method is designed to measure the degree of (dis)similarity among "variable grammars" of different dialects or varieties, where a variable grammar is understood as the set of constraints (a.k.a. predictors or "conditioning factors") governing the choice between two or more linguistic variants. Variants can be individual lexical items (*sneakers* vs. *trainers* vs. *tennis shoes*), grammatical constructions (*give me the book* vs. *give the book to me*), or phonetic realizations of a particular phoneme (e.g. [ʊ] vs. [ʌ] pronunciations of the **<span style="font-family:Times New Roman, Times, serif">STRUT</span>** vowel).  

The method takes inspiration from comparative sociolinguistics [see e.g. @Tagliamonte2013], which evaluates the relatedness between varieties and dialects based on how similar the conditioning of variation is in these varieties. Comparative sociolinguists rely on three lines of evidence to determine relatedness:  

1. Are the same constraints significant across varieties? 
2. Do the constraints have the same strength across varieties? 
3. Is the constraint hierarchy similar?

Below we'll go through the steps for calculating and assessing each of these lines of evidence. But first, we need to get the data into the necessary format. With VADIS we model particle placement across these nine varieties separately, so we first split the data up into nine individual datasets.

```{r}
# create list of dataframes
data_list <- split(pv, pv$Variety, drop = TRUE) # drop unused levels
names(data_list)
```

Now we're ready to begin.

## Step 1

In the first step we identify the most important constraints on variation. We know from prior research that many different factors influence the choice of particle placement, including:

- Length, definiteness, concreteness, and discourse accessibility of the direct object (*note personal pronoun direct objects are excluded from this dataset*)
- The semantic idiomaticity of the verb-particle
- The presence of a directional PP following the verb phrase 
- predictability of the particle given the verb, and the predictability of the verb given the particle
- Prior use of V-NP-P or V-P-NP orders
- The register, genre and/or mode (e.g. informal speech has much more V-NP-P tokens than formal writing) 

These are the predictors we'll include in our variable grammar models. We'll define our model formula using this set.

```{r}
f1 <- Response ~ DirObjWordLength + DirObjDefiniteness + DirObjGivenness + DirObjConcreteness + 
  DirObjThematicity + DirectionalPP + PrimeType + Semantics + Surprisal.P + Surprisal.V + Register
```

## Step 2

We now fit logistic regression models to the data (sub)sets. We'll use fixed-effects only models for this vignette since they are faster to compute, but I'll give an example of the code for a mixed-effects model below.

Next we simply loop over the datasets in `data_list` and fit a model to each one. Before fitting we'll center and rescale our model inputs [see @Gelman2008] with the `stand()` function. **Note:** Standardizing inputs in this way is essential for reliable calculation of the similarity coefficients in line 2 (Step 4). See `?stand` for details.

```{r}
glm_list <- vector("list")
for (i in seq_along(data_list)){
  d <- data_list[[i]]
  # now we'll standardize the model inputs (excluding the response) before fitting 
  d[all.vars(f1)[-1]] <- lapply(d[all.vars(f1)[-1]], FUN = stand)  
  glm_list[[i]] <- glm(f1, data = d, family = binomial, x = TRUE) # note the x = TRUE
}
names(glm_list) <- names(data_list) # add names to the list of models
```

Mixed-effects models would be run in much the same fashion. We just have to define a new formula and different modeling function. Since mixed-models take a bit longer to run, we'll only use fixed-effects models here.

```{r eval = F}
# update formula with by-verb and by-particle random intercepts 
f2 <- update(f1, .~ (1|Verb) + (1|Particle) + .)

glmer_list <- vector("list")
for (i in seq_along(data_list)){
  d <- data_list[[i]]
  # standardize the model inputs (excluding the response and random effects) before fitting
  d[all.vars(f2)[-c(1:3)]] <- lapply(d[all.vars(f2)[-c(1:3)]], FUN = stand) 
  # set optimizer controls to help convergence
  glmer_list[[i]] <- glmer(f2, data = d, family = binomial, 
    control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e7)))
}
names(glmer_list) <- names(data_list)
```

### Checking fits

Before moving on, we should check that the models fit the data well, and that we don't have any issues with collinearity or other signs of trouble. The `summary_stats()` function gives a number of statistics.

```{r}
summary_stats(glm_list) %>% 
  round(3)
```

- `N`: the total number of observations
- `baseline`: the baseline accuracy, i.e. the proportion of the most frequent response
- `predicted.corr`: the proportion of observations predicted correctly
- `C`: the concordance index *C*. Values below .8 suggest poor model fit
- `AIC`: the model's Akaike Information Criterion
- `Max.VIF`: the maximum variance inflation factor for the model's parameter estimates. Values above 10 suggest potentially troublesome collinearity among predictors
- `kappa`: the condition number indicating overall data multicollinearity. Values < 6 indicate no collinearity; ~15 indicate medium collinearity; > 30 indicate a worrysome degree of collinearity
- `overdisp.p`: the p-value of a test for overdispersion (greater variability in the response than expected for a binomial model). Low values (*p* < .05) indicate potential problems.



## Step 3 

In step 3 we look at the first line of evidence: "Are the same constraints significant across varieties?"

We do this with the `vadis_line1()` function. This function returns a list with three elements.

- A table `signif.table` of predictor significance values, including the intercept. 1 = significant, 0 = nonsignificant
- A distance matrix based on `signif.table`. For this function distances are [Hamming distances](https://en.wikipedia.org/wiki/Hamming_distance), which in the case of binary data is equivalent to the Manhattan or squared Euclidean distance. The intercept is excluded when calculating the distances. In such cases these distances are equivalent to the number of constraint significance values on which the two varieties disagree. These distances are then normalized by dividing by the total number of constraints, i.e. the maximum number of values on which two varieties could disagree.
- A table `similarity.coefs` of coefficients measured as the mean similarity for each variety to all the others.

The `path` argument tells the function where to save the output. The default setting saves a file `vadis_line1_output_2018-Oct-30_15.04.rds` to the current working directory. Note that the output of the function is a list object, so it saves it as an R data object file (`.rds`). Setting `path = FALSE` tells it not to save anything. 

```{r}
signif_line <- vadis_line1(glm_list, path = FALSE)
```

Examine each of the elements.

```{r}
signif_line$signif.table
```

```{r}
signif_line$distance.matrix
```

```{r}
signif_line$similarity.coefs
```

Individual components can be saved as .csv (or.txt) files like so.

```{r eval = F}
write.csv(signif_line$signif.table, 
          file = "line1_significance_table.csv")

write.csv(as.matrix(signif_line$distance.matrix), 
          file = "line1_distance_matrix.csv")

write.csv(signif_line$similarity.coefs, 
          file = "line1_similarity_coefs.csv")
```


## Step 4

In the next step we look at the second line of evidence. We do this with the `vadis_line2()` function. This function also returns a list with three elements.

- A table `coef.table` of predictor coefficients, including the intercept.
- A distance matrix based on `coef.table`. For this function distances are [Euclidean distances](https://en.wikipedia.org/wiki/Euclidean_distance), normalized to the maximum distance (see below). The intercept is excluded when calculating the distances.
- A table `similarity.coefs` of coefficients measured as the mean similarity for each variety to all the others. See @SzmrecsanyiInprep for description of how these are calculated.

```{r}
coef_line <- vadis_line2(glm_list, path = FALSE)
```

```{r}
coef_line$coef.table %>% 
  round(3)
```

```{r}
coef_line$distance.matrix %>% 
  round(3)
```

```{r}
coef_line$similarity.coefs
```


Note that both `vadis_line1()` and `vadis_line2()` work the same with `glm` and `glmer` models.

## Step 5

In this step we create the conditional random forest models from which we'll calculate the variable importance rankings for the third line of evidence. First we define the function for growing the forests. We'll use the `cforest()` function in `party` for this tutorial, but the package also supports random forests generated with `ranger`.

```{r}
crf_func <- function(d) {
  cforest(f1, d, controls = cforest_unbiased(ntree = 500, mtry = 3))
}
```

Now we loop over the datasets in `data_list` and apply the modelling function just like above.

```{r}
crf_list <- lapply(data_list, FUN = crf_func)
```

This gives us a list of random forests models similar to our list of regression models above.

### Checking fits

Before moving on, we should again check that the models fit the data well, and that we don't have any issues with collinearity or other signs of trouble. The `summary_stats()` function gives a subset of the statistics we saw before, mainly because random forest models don't provide us with these statistics. Additional arguments are also needed since the model objects don't contain the necessary information.

```{r}
summary_stats(crf_list, data_list, response = "Response") %>% 
  round(3)
```


## Step 6

In this next step we look at the third line of evidence. We do this with the `vadis_line3()` function. This function also returns a list with, you guessed it, three elements.

- A table `rank.table` of variable importance rankings.
- A distance matrix based on `rank.table`. For this function distances are measured by calculating the pairwise spearman rank correlation coefficients and subtracting from 1. The intercept is excluded when calculating the distances.
- A table `similarity.coefs` of mean similarity coeficients measured for each variety.

We then loop over the list of random forests and calculate the variable importance rankings. For `party` forests we use the `varimpAUC()` function, while `ranger` forests use the "permutation" method. With `party` forests we can also specify whether the rankings should be based on conditional importance scores, which are advised when there are many potentially correlated predictors [@Strobl2008]. Since conditional importance takes *much* longer to compute, we'll set it to `FALSE` (the default) here.

```{r}
varimp_line <- vadis_line3(crf_list, path = FALSE, conditional = FALSE)
```

```{r}
varimp_line$distance.matrix %>% 
  round(3)
```

```{r}
varimp_line$similarity.coefs
```


## Calculating maximum distance in line 2

In order to appropriately calculate line 2, we need to determine a maximum distance measure with which to normalize the pairwise distances. We define this distance based on what we know about the range of possible values the data is likely to have. We use the distance between two hypothetical varieties whose constraints have exactly the opposite effects. We additionally set the absolute size of all the constraints to a reasonable value to get two varieties that are about as far from one another as we could expect any two varieties to be. We note that this kind of constraint "flipping", i.e. the systematic reversal of the signs of the coefficients in two varieties, is very unlikely to happen in real world contexts, but that’s kind of the point. We are really really unlikely to come across a real world situation where two varieties’ grammars are more dissimilar than this.

For a comparison of grammars with *n* constraints, we define the maximum distance by taking the distance between two hypothetical varieties whose constraints all have an absolute effect size of ±1 but whose effect directions are the opposite of one another in the respective varities. For example, in a study comparing grammars with 10 constraints, we calculate the maximum reasonable distance by taking the euclidean distance between hypothetical varieties A and B that look like so.

```{r echo=F}
d <- data.frame(
  a = rep(c(-1,1), each = 5),
  b = rep(c(1,-1), each = 5))
```

The maximum distance in this case is therefore d(A,B) = `r round(as.numeric(dist(t(d), "euclidean")), 2)`. To normalize our distance matrices we divide the observed distances by this value to give distances within a range of 0 to 1. For the similarity coefficients we simply subtract the distances from 1 to give us a score where larger values represent greater average similarity.

We chose ±1 as the value for our hypothetical constraints since a change from -1 to 1 on the logodds scale represents an increase in probability of approximately 50 percentage points (logit^-1^(-1) = 0.27; logit^-1^(1) = 0.73). It seems very unlikely that we'd ever see such a big reversal in all constraint effects, if any, between two related varieties. Larger values we considered, e.g. ±2, but the larger the maximum distance, the less discrimination there is among the observed distances, and so we opted for slightly smaller values.

# Combining the 3 lines

We can merge the similarity scores across the three lines of evidence, and arrive at mean scores for each variety.

```{r}
mean_sims <- data.frame(
  line1 = signif_line$similarity.coefs,
  line2 = coef_line$similarity.coefs,
  line3 = varimp_line$similarity.coefs
)
mean_sims$mean <- apply(mean_sims, 1, mean)
mean_sims
```

From this we can calculate a mean of the mean similarity coefficients.

```{r}
mean(mean_sims$mean)
```

We can also create a combined distance matrix from the three lines of evidence. We use the `fuse()` function in the `analogue` package.

```{r}
fused_dist <- analogue::fuse(signif_line$distance.matrix, 
                             coef_line$distance.matrix, 
                             varimp_line$distance.matrix)
fused_dist
```


# Visualization

We now have our similarity coefficients and distance matrices, which we can visualize in a number of ways.


## Clustering

One way we can visualize distances is through clustering methods. There are many different functions and packages

```{r hclustplot, fig.height=6, fig.width=7}
line2_clust <- hclust(coef_line$distance.matrix, method = "ward.D2")
plot(line2_clust)
```

Other methods use divisive clustering (with `cluster::diana()`) or unrooted trees with neighbor joining methods (with `ape::nj()`).

```{r fig.height=6, fig.width=7}
cluster::diana(coef_line$distance.matrix) %>% 
  cluster::pltree()
```

```{r fig.height=6, fig.width=7}
ape::nj(coef_line$distance.matrix) %>% 
  plot(type = "u")
```




## Multidimensional scaling

Distance matrices can also be fed into a multidimensional scaling analysis.


```{r}
line2_mds <- cmdscale(coef_line$distance.matrix, k = 3, eig = T) 
```


```{r mdsplot, fig.height=6, fig.width=7}
line2_mds[[1]] %>%
  as.data.frame() %>% 
  mutate(genres = rownames(.)) %>% 
  ggplot(aes(V1, V2, label = genres)) +
  geom_point() +
  geom_text(nudge_y = .05, size = 4)
```

One nice thing about MDS maps is that they can be represented in 2 or 3 dimensions. There are a number of methods for generating 3D plots. The simplest is probably `scatterplot3d`.

```{r fig.height=6, fig.width=7}
dd <- line2_mds[[1]] %>%
  as.data.frame() 
library(scatterplot3d)
with(dd, {
  scttr <- scatterplot3d(x = V1, y = V2, z = V3, type = "h", pch = 18)
  scttr_coords <- scttr$xyz.convert(V1, V2, V3)
  text(scttr_coords$x, scttr_coords$y, labels = rownames(dd), pos = 3)
  })
```

`plotly` is another powerful package for generating all kinds of interactive graphics. 


```{r fig.height=6, fig.width=7}
library(plotly)
dd %>% 
  mutate(Variety = rownames(.)) %>% 
  plot_ly() %>%
  add_trace(x = ~V1, y = ~V2, z = ~V3,
            type = "scatter3d", inherit = F,
            marker = list(size = 4),
            mode = "markers") %>%
  add_text(x = ~V1, y = ~V2, z = ~V3,
           text = ~ Variety,
           type = "scatter3d",
           mode = "markers",
           showlegend = FALSE)
```



## NeighborNets

NeighborNet is yet another technique for constructing and visualizing phylogenetic relationships. The method is similar to other agglomerative clustering methods, e.g. neighbor-joining algorithms, but allows for "conflicting" signals. These conflicting signals are realized as web-like reticulations which indicate that the data support several possible tree structures [see @Dunn2008].  

```{r NNetplot, fig.height=6, fig.width=7}
line2_NNet <- phangorn::neighborNet(coef_line$distance.matrix)
plot(line2_NNet, "2D")
```


## References


